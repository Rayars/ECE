{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCEGeD1aZOo7ow7EpVWvUU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rayars/ECE/blob/main/BERT_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyG1IHMp9-h-",
        "outputId": "6a5d46ee-0feb-4c59-e7b6-c911d05610c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 39.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.2.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 25.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 21.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=da6beb2fef69f805c79fe13dbd9db3264944961fbec290b3cc2a2551d976cfee\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.18.0\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F   # 激励函数的库\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Zg5rk_nuK7g",
        "outputId": "5e1af612-278d-4bd8-afe1-e3f699acda1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义全局变量\n",
        "n_epochs = 10     # epoch 的数目\n",
        "batch_size = 20  # 决定每次读取多少图片\n",
        "\n",
        "class ECEDataset(Dataset):\n",
        "    def __init__(self, data_file, transform=None, target_transform=None):\n",
        "        self.data = pd.read_csv(data_file)\n",
        "        self.clauses=self.data.iloc[:,[0,1,2,3,5]]\n",
        "        self.labels=self.data.iloc[:,4]\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clauses)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clause = list(self.clauses.iloc[idx])\n",
        "        label = self.labels.iloc[idx]\n",
        "        if self.transform:\n",
        "            clause = self.transform(clause)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return clause, label\n",
        "\n",
        "#定义训练和数据集\n",
        "train_data,test_data=train_test_split(ECEDataset(\"/content/sample_data/clause_keywords.csv\"),test_size=0.2)\n",
        "#print(train_data[0],test_data[0])\n",
        "\n",
        "\n",
        "#创建加载器\n",
        "train_loader=torch.utils.data.DataLoader(train_data, batch_size = batch_size, num_workers = 0,shuffle=True)\n",
        "test_loader=torch.utils.data.DataLoader(test_data, batch_size = batch_size, num_workers = 0,shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "WRCTI2JzuTK4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立一个四层感知机网络\n",
        "class MLP(nn.Module):   # 继承 torch 的 Module\n",
        "    def __init__(self):\n",
        "        super(MLP,self).__init__()    # \n",
        "        # 初始化三层神经网络 两个全连接的隐藏层，一个输出层\n",
        "        self.fc1 = torch.nn.Linear(768,256)  # 第一个隐含层  \n",
        "        self.fc2 = torch.nn.Linear(256,64)  # 第二个隐含层\n",
        "        self.fc3 = torch.nn.Linear(64,32)   # 第三个隐藏层\n",
        "        self.fc4 = torch.nn.Linear(32,2)   #输出层\n",
        "\n",
        "        \n",
        "    def forward(self,din):\n",
        "        # 前向传播， 输入值：din, 返回值 dout\n",
        "        dout = F.relu(self.fc1(din))   # 使用 relu 激活函数\n",
        "        dout = F.relu(self.fc2(dout))\n",
        "        dout = F.relu(self.fc3(dout))\n",
        "        dout = F.softmax(self.fc4(dout), dim=1)  # 输出层使用 softmax 激活函数\n",
        "        # 2个数字实际上是2个类别，输出是概率分布，最后选取概率最大的作为预测值输出\n",
        "        return dout\n",
        "\n",
        "\n",
        "#建立Bbert+MLP网络\n",
        "class Sent_bert_MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sent_bert_MLP,self).__init__()\n",
        "        self.bert=SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "        self.MLP=MLP()\n",
        "\n",
        "    def forward(self,input):\n",
        "        sentence_embedding=self.bert.encode(input[4])\n",
        "        #print(input[4])\n",
        "        #print(sentence_embedding.shape)\n",
        "        dout=self.MLP(torch.tensor(sentence_embedding))\n",
        "        return dout\n"
      ],
      "metadata": {
        "id": "ZEMip4h1LUuW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5x7yxz4bdENK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b6aeb8-70f6-446c-8efe-148c734035dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1252/1252 [01:38<00:00, 12.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.628371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:24<00:00, 12.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test clause: 6.789137 %\n",
            "Epoch:  2  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1252/1252 [01:38<00:00, 12.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.597985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:24<00:00, 12.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test clause: 17.252396 %\n",
            "Epoch:  3  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1252/1252 [01:39<00:00, 12.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.588143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:24<00:00, 12.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test clause: 24.680511 %\n",
            "Epoch:  4  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1252/1252 [01:39<00:00, 12.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.590327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:24<00:00, 12.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test clause: 26.086262 %\n",
            "Epoch:  5  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1252/1252 [01:39<00:00, 12.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.588485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:24<00:00, 12.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test clause: 37.300319 %\n",
            "Epoch:  6  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1252/1252 [01:39<00:00, 12.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.579054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:24<00:00, 12.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test clause: 47.683706 %\n",
            "Epoch:  7  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1252/1252 [01:39<00:00, 12.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.581925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:24<00:00, 12.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test clause: 32.348243 %\n",
            "Epoch:  8  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1252/1252 [01:39<00:00, 12.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.580222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:24<00:00, 12.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test clause: 14.520767 %\n",
            "Epoch:  9  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1252/1252 [01:39<00:00, 12.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.574996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:24<00:00, 12.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test clause: 23.961661 %\n",
            "Epoch:  10  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1252/1252 [01:39<00:00, 12.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.575216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:24<00:00, 12.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test clause: 32.092652 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bert=SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# 训练神经网络\n",
        "def train():\n",
        "    #定义损失函数和优化器\n",
        "    lossfunc = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([1.0,99.0])).float())\n",
        "    lossfunc.cuda()\n",
        "    optimizer = torch.optim.SGD(params = model.parameters(), lr = 0.01)\n",
        "    # 开始训练\n",
        "    for epoch in range(n_epochs):\n",
        "        print('Epoch:  {}  \\t'.format(epoch+1))\n",
        "        train_loss = 0.0\n",
        "        for data,target in tqdm(train_loader):\n",
        "            data=torch.tensor(bert.encode(data[4])) # embedding\n",
        "            target=list(target)\n",
        "            for i in range(len(target)):\n",
        "              if target[i]=='yes':\n",
        "                target[i]=1\n",
        "              else:\n",
        "                target[i]=0\n",
        "            target=torch.tensor(target)\n",
        "            data,target=data.to(device),target.to(device)\n",
        "            optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
        "            output = model(data)    # 得到预测值\n",
        "            loss = lossfunc(output,target)  # 计算两者的误差\n",
        "            loss.backward()         # 误差反向传播, 计算参数更新值\n",
        "            optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n",
        "            train_loss += loss.item()*data.size(0)\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        print('Training Loss: {:.6f}'.format( train_loss))\n",
        "        # 每遍历一遍数据集，测试一下准确率\n",
        "        test()\n",
        "\n",
        "# 在数据集上测试神经网络\n",
        "def test():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # 训练集中不需要反向传播\n",
        "        for data,target in tqdm(test_loader):\n",
        "            data=torch.tensor(bert.encode(data[4])) # embedding\n",
        "            target=list(target)\n",
        "            for i in range(len(target)):\n",
        "              if target[i]=='yes':\n",
        "                target[i]=1\n",
        "              else:\n",
        "                target[i]=0\n",
        "            target=torch.tensor(target)\n",
        "            data,target=data.to(device),target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    print('Accuracy of the network on the test clause: %f %%' % (\n",
        "        100.0 * correct / total))\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "# 声明感知器网络\n",
        "model = MLP()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#print(device)\n",
        "model.to(device)\n",
        "train()"
      ]
    }
  ]
}