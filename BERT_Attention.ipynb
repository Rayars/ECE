{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNprTQV6JhY5x3zPZog0cc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rayars/ECE/blob/main/BERT_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzHBgcAByIB7",
        "outputId": "ee54a455-29ae-4152-cca0-1503ba5b3180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.19.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F   # 激励函数的库\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import random \n",
        "import math"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeA_X_Lu1jfW",
        "outputId": "9035bd6f-2158-4fc9-c368-7f1c7bf9412d"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#常用函数\n",
        "def label_translate(target):\n",
        "  target=list(target)\n",
        "  for i in range(len(target)):\n",
        "    if target[i]=='yes':\n",
        "      target[i]=[1.0,0.0]\n",
        "    else:\n",
        "      target[i]=[0.0,1.0]\n",
        "  target=torch.tensor(target)\n",
        "  return target\n",
        "\n",
        "def sentence_embedding(sent):#对整个句子做嵌入，返回子句向量的list\n",
        "  sentence=[]\n",
        "  bert=SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "  for c in sent:\n",
        "    v=bert.encode(c)\n",
        "    sentence.append(v)\n",
        "  return sentence\n",
        "\n",
        "def positoin_embedding(clause):\n",
        "  pass"
      ],
      "metadata": {
        "id": "O0al43w8Yyc3"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义全局变量\n",
        "n_epochs = 10     # epoch 的数目\n",
        "batch_size = 20  # 决定每次读取多少样本\n",
        "\n",
        "class ECEDataset_sent(Dataset): #每次读取一个句子\n",
        "    def __init__(self, data_file, transform=None, target_transform=None):\n",
        "        self.data = pd.read_csv(data_file)\n",
        "        self.sent_data=self.data.groupby('sent_num')\n",
        "        self.clauses,self.labels=[],[]\n",
        "        for i in self.sent_data:\n",
        "          self.clauses.append(i[1])\n",
        "          self.labels.append(i[1]['label'])\n",
        "        self.length=len(self.clauses)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        clause=list(self.clauses[idx]['text'].values)\n",
        "        clause += [\"\" for i in range(50-len(clause))]\n",
        "        label=list(self.labels[idx].values)\n",
        "        label += [\"no\" for i in range(50-len(label))]\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            clause = self.transform(clause)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return clause, label\n",
        "\n",
        "# 定义训练和数据集，以句子为单位\n",
        "data=ECEDataset_sent(\"/content/sample_data/clause_keywords.csv\")\n",
        "\n",
        "#划分训练集和测试集\n",
        "train_data,test_data=train_test_split(data,train_size=0.8)\n",
        "print(train_data[0],\"\\n\",test_data[0])\n",
        "\n",
        "\n",
        "# 创建加载器\n",
        "train_loader=torch.utils.data.DataLoader(train_data, batch_size = batch_size, num_workers = 2,shuffle=True)\n",
        "test_loader=torch.utils.data.DataLoader(test_data, batch_size = batch_size, num_workers = 2,shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ifGTWoe1lW1",
        "outputId": "87814b10-a3c2-4207-c1f3-90f3aaf6f93c"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['儿子 贪玩 狼爸 拿 竹条 打死 他 案情 永川 的 李某 离婚 后', '带 着 儿子 小鹏 化名 来到 主城 打工', '平时', '李某 一家 靠 制作 销售 豆腐 豆芽菜 维持 生计', '对 儿子 缺少 管护', '贪玩 的 小鹏 让 李某 很 头痛', '1999 年 10 月 15 日晚', '小鹏 放学 后 一直 没 回家', '李某 就 到 儿子 就读 的 学校 附近 寻找', '当晚 8 时许', '李某 找到 儿子', '回家 后', '为 教训 儿子', '李某 把 小鹏 捆绑 在 屋外 院坝 猪圈 横梁 上', '用 竹条 长时间 持续 抽打 儿子 下腹部 背 臀部 等 处', '当晚 10 时许', '小鹏 死亡', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no']) \n",
            " (['实际上', '她 这样 做 我 还是 愿意 理解 的', '我 只是 想 不 明白', '为什么 会 这样', '前天', '女方 家长 说 — — 已经 过去 了 半个 多月', '事情 真相 仍然 扑朔迷离', '前天 下午', '为了 弄个 明白', '葵氏 父子 来到 了 郑家', '郑父 说', '出 了 这样 的 事情', '自己 和 妻子 也 对 女儿 感到 十分 失望', '但 女儿 已经 10 多天 没 给 家里 打 过 任何 电话', '父母 试图 找 她', '也 同样 联系 不上', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练神经网络\n",
        "def train():\n",
        "    #定义损失函数和优化器\n",
        "    lossfunc = nn.CrossEntropyLoss()\n",
        "    lossfunc.cuda()\n",
        "    optimizer = torch.optim.SGD(params = model.parameters(), lr = 0.01)\n",
        "    # 开始训练\n",
        "    for epoch in range(n_epochs):\n",
        "        print('Epoch:  {}  \\t'.format(epoch+1))\n",
        "        train_loss = 0.0\n",
        "        for data,target in tqdm(train_loader):\n",
        "            tran_data,tran_target=[],[]\n",
        "            for i in range(batch_size):#经过这个双重循环可以将dataloader取出的数据转置\n",
        "              sentence=[]\n",
        "              label=[]\n",
        "              for d in data:\n",
        "                sentence.append(d[i])\n",
        "              for l in target:\n",
        "                label.append(l[i])\n",
        "              tran_data.append(sentence)\n",
        "              tran_target.append(label)\n",
        "            data,target=tran_data,tran_target\n",
        "            keys=data\n",
        "            for i in range(batch_size):\n",
        "              keys[i]=sentence_embedding(keys[i])\n",
        "            keys=torch.stack(keys,0)\n",
        "            querys=keys\n",
        "            newtarget=[]\n",
        "            for t in target:\n",
        "              newtarget.append(label_translate(t))\n",
        "            target=torch.stack(newtarget,0)\n",
        "            querys,keys,target=querys.to(device),keys.to(device),target.to(device)\n",
        "            optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
        "\n",
        "            \n",
        "            output = model(querys,keys,keys)    # 得到预测值 MLP考虑batch_size\n",
        "            print(output,target)\n",
        "            print(type(output),type(target))\n",
        "            print(len(output),len(target))\n",
        "            print(output[0],target[0])\n",
        "            for i in range(len(output)):\n",
        "              loss = lossfunc(output[i].squeeze(0),target[i])  # 计算两者的误差\n",
        "              loss.backward()         # 误差反向传播, 计算参数更新值\n",
        "              optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n",
        "              train_loss += loss.item()*data.size(0)\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        print('Training Loss: {:.6f}'.format( train_loss))\n",
        "        # 每遍历一遍数据集，测试一下准确率\n",
        "        test()\n",
        "\n",
        "# 在数据集上测试神经网络\n",
        "def test():\n",
        "    TP,FN,FP,TN=0,0,0,0\n",
        "    P,R,F = 0,0,0\n",
        "    with torch.no_grad():  # 训练集中不需要反向传播\n",
        "        for data in tqdm(test_loader):\n",
        "            keys=data[0]['text'].values\n",
        "            querys=keys\n",
        "            querys,keys=sentence_embedding(querys),sentence_embedding(keys)\n",
        "            target=label_translate(list(data[1].value))\n",
        "            querys,keys,target=querys.squeeze(0).to(device),keys.squeeze(0).to(device),target.squeeze(0).to(device)\n",
        "            outputs = model(querys,keys,keys) #outputs: [[0.7,0.3],[0.2,0.8]...]\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            for i in range(len(predicted)):\n",
        "              if predicted[i] == 1 and target[i] == 1 :\n",
        "                TP+=1\n",
        "              elif predicted[i] == 0 and target[i] == 1:\n",
        "                FN+=1\n",
        "              elif predicted[i] == 1 and target[i] == 0:\n",
        "                FP+=1\n",
        "              elif predicted[i] == 0 and target[i] == 0:\n",
        "                TN+=1\n",
        "            P=TP/(TP+FP)\n",
        "            R=TP/(TP+FN)\n",
        "            F=2/(1/P+1/R)\n",
        "    print('Pricision, Recall and F of the network on the test clause: %f %%, %f %%, %f %%' % (\n",
        "        100.0 * P,100.0*R,100.0*F))\n",
        "    return 100.0 * P,100.0*R,100.0*F\n",
        "\n",
        "# 声明感知器网络\n",
        "model = Attention_MLP(768,1,1)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#print(device)\n",
        "model.to(device)\n",
        "train()"
      ],
      "metadata": {
        "id": "IE8nXiCq1rYH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e801c40-19fb-49c9-a3e9-411875b68be1"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1  \t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/85 [01:44<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 50, 768])\n",
            "[tensor([0.4868, 0.5132], grad_fn=<SqueezeBackward1>), tensor([0.4873, 0.5127], grad_fn=<SqueezeBackward1>), tensor([0.4870, 0.5130], grad_fn=<SqueezeBackward1>), tensor([0.4866, 0.5134], grad_fn=<SqueezeBackward1>), tensor([0.4873, 0.5127], grad_fn=<SqueezeBackward1>), tensor([0.4867, 0.5133], grad_fn=<SqueezeBackward1>), tensor([0.4871, 0.5129], grad_fn=<SqueezeBackward1>), tensor([0.4868, 0.5132], grad_fn=<SqueezeBackward1>), tensor([0.4863, 0.5137], grad_fn=<SqueezeBackward1>), tensor([0.4853, 0.5147], grad_fn=<SqueezeBackward1>), tensor([0.4859, 0.5141], grad_fn=<SqueezeBackward1>), tensor([0.4876, 0.5124], grad_fn=<SqueezeBackward1>), tensor([0.4870, 0.5130], grad_fn=<SqueezeBackward1>), tensor([0.4871, 0.5129], grad_fn=<SqueezeBackward1>), tensor([0.4863, 0.5137], grad_fn=<SqueezeBackward1>), tensor([0.4876, 0.5124], grad_fn=<SqueezeBackward1>), tensor([0.4877, 0.5123], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>), tensor([0.4891, 0.5109], grad_fn=<SqueezeBackward1>)] tensor([[[0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         ...,\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.]],\n",
            "\n",
            "        [[0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         ...,\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.]],\n",
            "\n",
            "        [[1., 0.],\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         ...,\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         ...,\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.]],\n",
            "\n",
            "        [[0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         ...,\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.]],\n",
            "\n",
            "        [[0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         ...,\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.]]])\n",
            "<class 'list'> <class 'torch.Tensor'>\n",
            "50 20\n",
            "tensor([0.4868, 0.5132], grad_fn=<SqueezeBackward1>) tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-163-2f3ebaa1ae24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m#print(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-163-2f3ebaa1ae24>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 计算两者的误差\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# 误差反向传播, 计算参数更新值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# 将参数更新值施加到 net 的 parameters 上\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1164\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2995\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2996\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 感知机网络\n",
        "class MLP(nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(MLP,self).__init__()   \n",
        "        \n",
        "        self.fc1 = torch.nn.Linear(768,256)  \n",
        "        self.fc2 = torch.nn.Linear(256,128)  \n",
        "        self.fc3 = torch.nn.Linear(128,64)   \n",
        "        self.fc4 = torch.nn.Linear(64,32)   \n",
        "        self.fc5 = torch.nn.Linear(32,8)\n",
        "        self.fc6 = torch.nn.Linear(8,2)\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self,din):\n",
        "        # 前向传播， 输入值：din, 返回值 dout\n",
        "        dout = F.relu(self.fc1(din))   # 使用 relu 激活函数\n",
        "        dout = F.relu(self.fc2(dout))\n",
        "        dout = F.relu(self.fc3(dout))\n",
        "        dout = F.relu(self.fc4(dout))\n",
        "        dout = F.relu(self.fc5(dout))\n",
        "        dout = F.softmax(self.fc6(dout), dim=1)  # 输出层使用 softmax 激活函数\n",
        "        # 2个数字实际上是2个类别，输出是概率分布，最后选取概率最大的作为预测值输出\n",
        "        return dout"
      ],
      "metadata": {
        "id": "ZEMip4h1LUuW"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dot_attention(nn.Module):\n",
        "    \"\"\" 点积注意力机制\"\"\"\n",
        "\n",
        "    def __init__(self, attention_dropout=0.0):\n",
        "        super(dot_attention, self).__init__()\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, q, k, v, scale=None, attn_mask=None):\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        :param q:\n",
        "        :param k:\n",
        "        :param v:\n",
        "        :param scale:\n",
        "        :param attn_mask:\n",
        "        :return: 上下文张量和attention张量。\n",
        "        \"\"\"\n",
        "        attention = torch.bmm(q, k.transpose(1, 2))\n",
        "        if scale:\n",
        "            attention = attention * scale        # 是否设置缩放\n",
        "        if attn_mask:\n",
        "            attention = attention.masked_fill(attn_mask, -np.inf)     # 给需要mask的地方设置一个负无穷。\n",
        "        # 计算softmax\n",
        "        attention = self.softmax(attention)\n",
        "        # 添加dropout\n",
        "        attention = self.dropout(attention)\n",
        "        # 和v做点积。\n",
        "        context = torch.bmm(attention, v)\n",
        "        return context, attention"
      ],
      "metadata": {
        "id": "NouKdyg3o5ku"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#多头自注意力机制 \n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" 多头自注意力\"\"\"\n",
        "    def __init__(self, model_dim=768, num_heads=2, dropout=0.0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.dim_per_head = model_dim//num_heads   # 每个头的维度\n",
        "        self.num_heads = num_heads\n",
        "        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
        "        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
        "        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
        "\n",
        "        self.dot_product_attention = dot_attention(dropout)\n",
        "\n",
        "        self.linear_final = nn.Linear(model_dim, model_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(model_dim)         # LayerNorm 归一化。\n",
        "\n",
        "    def forward(self, key, value, query, attn_mask=None):\n",
        "        # 残差连接\n",
        "        residual = query\n",
        "\n",
        "        dim_per_head = self.dim_per_head\n",
        "        num_heads = self.num_heads\n",
        "        batch_size = key.size(0)\n",
        "\n",
        "        # 线性映射。\n",
        "        key = self.linear_k(key)\n",
        "        value = self.linear_v(value)\n",
        "        query = self.linear_q(query)\n",
        "\n",
        "        # 按照头进行分割\n",
        "        key = key.view(batch_size * num_heads, -1, dim_per_head)\n",
        "        value = value.view(batch_size * num_heads, -1, dim_per_head)\n",
        "        query = query.view(batch_size * num_heads, -1, dim_per_head)\n",
        "\n",
        "        if attn_mask:\n",
        "            attn_mask = attn_mask.repeat(num_heads, 1, 1)\n",
        "\n",
        "        # 缩放点击注意力机制\n",
        "        scale = (key.size(-1) // num_heads) ** -0.5\n",
        "        context, attention = self.dot_product_attention(query, key, value, scale, attn_mask)\n",
        "\n",
        "        # 进行头合并 concat heads\n",
        "        context = context.view(batch_size, -1, dim_per_head * num_heads)\n",
        "\n",
        "        # 进行线性映射\n",
        "        output = self.linear_final(context)\n",
        "\n",
        "        # dropout\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        # 添加残差层和正则化层。\n",
        "        output = self.layer_norm(residual + output)\n",
        "\n",
        "        return output, attention\n",
        "#BERT_MultiHeadSelfAttention模型\n",
        "class Attention_MLP(nn.Module):\n",
        "\n",
        "    def __init__(self,model_dim=768, num_heads=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.attention=MultiHeadAttention(model_dim=768, num_heads=1, dropout=0.0)\n",
        "        self.MLP=MLP()\n",
        "\n",
        "    def forward(self,query,key,value):\n",
        "        output,attention=self.attention.forward(key,query,value)\n",
        "        print(output.shape)\n",
        "        result=[]\n",
        "        for clause in output[0].squeeze(0):\n",
        "          result.append(self.MLP(clause.unsqueeze(0)).squeeze(0))     \n",
        "        return result"
      ],
      "metadata": {
        "id": "WUD9wAI_RPnu"
      },
      "execution_count": 126,
      "outputs": []
    }
  ]
}